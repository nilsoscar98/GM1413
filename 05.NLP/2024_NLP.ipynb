{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f4af22-2f91-4c1e-8f4e-f41516cf05c1",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "## What is NLP?\n",
    "\n",
    "Natural Language Processing (NLP) is a field of computer science and artificial intelligence focused on the interaction between computers and human language. The goal of NLP is to enable machines to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
    "\n",
    "In simpler terms, NLP is the technology behind tools that allow computers to process and analyze large amounts of natural language data, such as written texts or spoken words.\n",
    "\n",
    "---\n",
    "\n",
    "## Why is NLP important in Finance and Economics?\n",
    "\n",
    "In finance and economics, large volumes of textual data are generated daily, such as:\n",
    "- Financial reports\n",
    "- Earnings calls transcripts\n",
    "- News articles\n",
    "- Social media discussions (tweets, blogs, etc.)\n",
    "- Regulatory filings\n",
    "\n",
    "These texts often contain important information about companies, markets, and the economy, but manually analyzing them can be time-consuming and difficult. NLP can help by automatically extracting insights from this text data, including sentiment (positive or negative tone), key terms, and trends.\n",
    "\n",
    "**Examples of NLP in Finance and Economics:**\n",
    "- **Sentiment Analysis**: Analyzing the tone of financial reports or news articles to understand market sentiment. For instance, whether the market is optimistic or pessimistic about a company’s future performance.\n",
    "- **Named Entity Recognition (NER)**: Identifying important entities such as company names, dates, locations, or products from text documents.\n",
    "- **Text Summarization**: Automatically summarizing long financial reports or regulatory filings.\n",
    "\n",
    "In this notebook, we will focus on one specific task of NLP: **sentiment analysis using a dictionary-based method**. This will allow us to analyze the sentiment (positive or negative) in financial texts based on predefined word lists (dictionaries).\n",
    "\n",
    "---\n",
    "\n",
    "# Getting Text Data\n",
    "\n",
    "Before we can perform any analysis on text, we need to gather or generate text data. In real-world scenarios, this data often comes from a variety of sources such as news articles, financial reports, or transcripts of company earnings calls. However, for this tutorial, we'll focus on working with simple text data that we generate ourselves.\n",
    "\n",
    "The goal is to get the text into a structured format so that we can process it using NLP techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Text Data in Finance and Economics\n",
    "\n",
    "Text data in finance and economics can come from various sources, each with unique characteristics:\n",
    "- **News Articles**: Articles from financial news outlets (e.g., Bloomberg, Reuters) often discuss trends in the market or specific company performance.\n",
    "- **Financial Reports**: Documents like quarterly earnings reports (10-Q) or annual reports (10-K) provide detailed insights into a company's financial performance.\n",
    "- **Earnings Calls Transcripts**: Transcripts of calls where company executives discuss financial performance with analysts and investors.\n",
    "- **Regulatory Filings**: Documents filed with regulatory authorities, such as the Securities and Exchange Commission (SEC), often include valuable information about a company’s operations.\n",
    "- **Social Media and Blogs**: Discussions and opinions about market trends and specific companies can be found on platforms like Twitter, Reddit, and financial blogs.\n",
    "\n",
    "In practice, text data may need to be extracted from these sources. For the purpose of this notebook, we will not focus on the extraction part (such as converting PDF reports into text), and instead assume that we already have the text available for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Simple Finance Text\n",
    "\n",
    "Here is a basic example of a piece of text data related to finance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1189359b-481f-4508-a430-a84453cdc41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_text = \"\"\"\n",
    "The company reported a significant increase in quarterly profits, \n",
    "but its debt levels have also risen. Analysts are concerned about \n",
    "the growing debt but remain optimistic about the company's overall growth potential.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e7b8b2-c6f6-4c91-8cee-8331ff789f83",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "- The introduction provides a broad overview of NLP and how it applies to finance/economics, written for a non-technical audience.\n",
    "- In \"Getting Text Data,\" the notebook guides the students through the types of textual data that exist in finance/economics, helping them understand where this data comes from.\n",
    "- The example text shows how to generate a simple finance-related text for the analysis.\n",
    "\n",
    "Once you’re ready, I can proceed with writing the text for the next section, **Basic Text Preprocessing**! Let me know if you'd like any modifications or further details.\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54532c95-29f7-4d88-b997-3da4f05ca899",
   "metadata": {},
   "source": [
    "# 1. Basic Text Preprocessing\n",
    "\n",
    "Before we can analyze text using a dictionary-based sentiment analysis method, we need to preprocess the raw text. Preprocessing helps standardize the text and remove irrelevant information, making it easier to match words in the text with entries in our sentiment dictionary.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Preprocess Text?\n",
    "\n",
    "Raw text data can be messy and inconsistent. For example, the same word might appear in different forms (e.g., \"Profit\" vs. \"profit\"), or the text may contain irrelevant characters (e.g., punctuation) that we don’t want to include in our analysis. \n",
    "\n",
    "By preprocessing the text, we can clean it up and convert it into a form that is easier to work with. The preprocessing steps we will cover are:\n",
    "- Lowercasing\n",
    "- Removing punctuation and special characters\n",
    "- Removing stop words\n",
    "- Lemmatization\n",
    "\n",
    "Let's walk through each of these steps in detail.\n",
    "\n",
    "---\n",
    "\n",
    "## Lowercasing\n",
    "\n",
    "One of the simplest but most important preprocessing steps is converting all text to lowercase. This ensures that words like \"Profit\" and \"profit\" are treated the same, preventing case sensitivity from affecting the analysis.\n",
    "\n",
    "### Example: Converting text to lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b34cff3-4958-4626-b475-ed9785e43504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the company reported a significant increase in quarterly profits, \n",
      "but its debt levels have also risen. analysts are concerned about \n",
      "the growing debt but remain optimistic about the company's overall growth potential.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert text to lowercase\n",
    "lowercased_text = finance_text.lower()\n",
    "print(lowercased_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bc02d8-7626-4404-ae1f-89207aa042f2",
   "metadata": {},
   "source": [
    "# 2. Removing Punctuation and Special Characters\n",
    "Punctuation marks (such as periods, commas, and exclamation points) don’t provide useful information for sentiment analysis, so we remove them. The same goes for special characters, such as dollar signs ($) or percentages (%), which aren’t relevant for dictionary-based analysis.\n",
    "\n",
    "## Example: Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283dabe0-ea4e-469f-9602-d53825314503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the company reported a significant increase in quarterly profits \n",
      "but its debt levels have also risen analysts are concerned about \n",
      "the growing debt but remain optimistic about the companys overall growth potential\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Removing punctuation\n",
    "no_punctuation_text = lowercased_text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(no_punctuation_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da75cc8e-ee2b-4e54-a9a0-fb992fd2a555",
   "metadata": {},
   "source": [
    "This removes all punctuation marks, leaving us with only words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc3f1d-a977-4362-971c-ace30ad2618e",
   "metadata": {},
   "source": [
    "# 3. Stop Words\n",
    "Stop words are common words like \"and,\" \"the,\" \"is,\" and \"in\" that are often considered irrelevant in text analysis because they don’t convey much meaning. In financial texts, we also often filter out domain-specific stop words (such as “company” or “business”) that do not contribute to sentiment.\n",
    "\n",
    "Using a pre-defined list of stop words, we can remove these words from our text to focus on the more meaningful terms.\n",
    "\n",
    "## Example: Removing stop words using spaCy\n",
    "We will use the spaCy library’s built-in list of stop words to remove them from our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5d64b77-0ccb-4b17-b1b2-860b20be4bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy pandas numpy --upgrade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2137ac0c-e03e-4478-a3cf-bb68fa0fffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you encounter an issue related to spaCy, particularly one involving missing data or models, \n",
    "# you might need to download the English model explicitly. To do this, uncomment and run the following command:\n",
    "# !python -m spacy download en_core_web_md !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dcdd450-9a90-4c25-a29b-85fc752e9f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/src/.local/lib/python3.8/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_md' (3.1.0) was trained with spaCy v3.1.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " company reported significant increase quarterly profits \n",
      " debt levels risen analysts concerned \n",
      " growing debt remain optimistic companys overall growth potential \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Convert text to spaCy Doc object\n",
    "doc = nlp(no_punctuation_text)\n",
    "\n",
    "# Removing stop words\n",
    "no_stop_words_text = ' '.join([token.text for token in doc if not token.is_stop])\n",
    "print(no_stop_words_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be18fe57-4339-4ecf-a879-99ef10b9b59c",
   "metadata": {},
   "source": [
    "In this step, all common stop words are removed, leaving us with only the more significant words for analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757981df-a618-4833-8ade-ef526b0daca4",
   "metadata": {},
   "source": [
    "# 4. Lemmatization\n",
    "Lemmatization is the process of converting words to their base or root form. For example, the words \"running,\" \"ran,\" and \"runs\" are all forms of the word \"run.\" By converting words to their base form, we ensure that we capture the meaning of the word, regardless of its tense or form.\n",
    "\n",
    "## Example: Lemmatizing text using spaCy\n",
    "We will use spaCy’s built-in lemmatizer to convert words in the text to their root forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2f8b30-c4f5-426c-9318-70b31d610715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " company report significant increase quarterly profit \n",
      " debt level rise analyst concerned \n",
      " grow debt remain optimistic company overall growth potential \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization of the remaining text\n",
    "lemmatized_text = ' '.join([token.lemma_ for token in doc if not token.is_stop])\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6a906-d20b-430d-a280-3ee24f92650d",
   "metadata": {},
   "source": [
    "# Final Preprocessed Text\n",
    "After applying all the preprocessing steps (lowercasing, removing punctuation, removing stop words, and lemmatization), we now have a clean version of the text that is ready for sentiment analysis using a dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a75db452-9b10-4e5d-a577-2efbbd4fb503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " company report significant increase quarterly profit \n",
      " debt level rise analyst concerned \n",
      " grow debt remain optimistic company overall growth potential \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final Preprocessed Text\n",
    "print(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f482e-a1db-4fcd-a3dd-a03a592ee34e",
   "metadata": {},
   "source": [
    "At this stage, our text is in a standardized form, with all unnecessary elements removed. We can now proceed to the next step: applying a sentiment dictionary to this text.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "- **Lowercasing**: Explained in simple terms why converting text to lowercase is important. The code converts text to lowercase using Python’s `lower()` method.\n",
    "- **Removing Punctuation**: The code example uses Python’s `string.punctuation` and `str.maketrans()` to remove punctuation from the text.\n",
    "- **Stop Words**: Introduced the concept of stop words, followed by code using `spaCy` to remove them. The explanation emphasizes how this helps focus on the more meaningful words in text.\n",
    "- **Lemmatization**: Explained why lemmatization is important and how it helps in simplifying words for analysis. The code example shows how to use spaCy’s lemmatizer.\n",
    "\n",
    "With this text, we have completed the preprocessing steps necessary to prepare text for analysis. Let me know if you'd like any adjustments or further details before I proceed with the next section on **Sentiment Scoring Using a Dictionary**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee57e5-61de-488f-8f5e-d11acc358a8f",
   "metadata": {},
   "source": [
    "# 5. Sentiment Scoring Using a Dictionary\n",
    "\n",
    "Now that we have preprocessed the text, we can move on to **sentiment scoring**. This process involves using a **sentiment dictionary** to calculate the overall tone (positive or negative) of the text. In our case, we will use a dummy dictionary with predefined **positive** and **negative** words, and we'll count how many times each of these words appears in the text.\n",
    "\n",
    "---\n",
    "\n",
    "## How Does Dictionary-Based Sentiment Analysis Work?\n",
    "\n",
    "A **sentiment dictionary** is a predefined list of words that are labeled as either positive or negative. Each time a word from the text appears in the dictionary, it contributes to the sentiment score. For example:\n",
    "- **Positive words** (e.g., \"profit,\" \"growth,\" \"success\") contribute positively to the sentiment score.\n",
    "- **Negative words** (e.g., \"loss,\" \"debt,\" \"decline\") contribute negatively to the sentiment score.\n",
    "\n",
    "The final sentiment score is the difference between the counts of positive and negative words in the text.\n",
    "\n",
    "In this section, we will:\n",
    "1. Define a dummy dictionary of positive and negative words.\n",
    "2. Use a dummy earnings call transcript to calculate a sentiment score.\n",
    "3. Apply basic counting techniques using sets.\n",
    "\n",
    "---\n",
    "\n",
    "## Dummy Earnings Call Transcript\n",
    "\n",
    "We will start by creating a simple dummy text that mimics the kind of language used in an earnings call or financial report. This will allow us to demonstrate how the sentiment analysis works in a financial context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e94dc74-dd83-4073-b834-fcc18347b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy earnings call transcript\n",
    "earnings_call_text = \"\"\"\n",
    "The company has experienced strong growth this quarter with increasing profits. \n",
    "Our revenue has risen significantly, and we are optimistic about future success. \n",
    "However, debt levels have also increased, and there are concerns about rising costs.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81e709-f8c7-44fd-bcc7-159bc589e357",
   "metadata": {},
   "source": [
    "This text contains a mix of positive and negative words, which we will analyze using a predefined dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f12e2-93e2-4bc1-a817-690efffc5ae2",
   "metadata": {},
   "source": [
    "# Creating a Dummy Sentiment Dictionary\n",
    "Here’s a simple example of a sentiment dictionary. We will create two sets of words:\n",
    "\n",
    "- Positive words: Words that convey positive sentiment in financial contexts.\n",
    "- Negative words: Words that convey negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a26938f4-0b94-4ead-9dac-12255772224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dummy sentiment dictionary\n",
    "positive_words = {\"growth\", \"profit\", \"revenue\", \"optimistic\", \"success\", \"increase\", \"risen\"}\n",
    "negative_words = {\"debt\", \"loss\", \"decline\", \"concerns\", \"decreased\", \"costs\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992da45d-cc5b-4ea7-baf6-79aa7267e163",
   "metadata": {},
   "source": [
    "These words are common in financial contexts and represent both positive and negative sentiments that might appear in an earnings call transcript.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceadc579-cd95-4e2c-a63f-258e6d0ee564",
   "metadata": {},
   "source": [
    "# Preprocessing the Text\n",
    "Before we can score the sentiment, we need to preprocess the text using the steps we outlined earlier (lowercasing, removing punctuation, removing stop words, and lemmatization). This ensures that the text is in a clean format.\n",
    "\n",
    "## Preprocessing the Dummy Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55af7faa-be41-477f-8e0a-3ddc3edca548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " company experience strong growth quarter increase profit \n",
      " revenue rise significantly optimistic future success \n",
      " debt level increase concern rise cost \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text (following the same steps as before)\n",
    "\n",
    "# Convert to lowercase\n",
    "lowercased_text = earnings_call_text.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "import string\n",
    "no_punctuation_text = lowercased_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Load spaCy's English language model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Convert to spaCy Doc object and remove stop words\n",
    "doc = nlp(no_punctuation_text)\n",
    "preprocessed_text = ' '.join([token.lemma_ for token in doc if not token.is_stop])\n",
    "\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af6d0d-1dc6-4428-8956-ae4cdec83811",
   "metadata": {},
   "source": [
    "This gives us a preprocessed version of the earnings call transcript that is ready for sentiment analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466eb1b6-3b37-4dfc-bb9a-b26fd418357e",
   "metadata": {},
   "source": [
    "# Counting Positive and Negative Words\n",
    "Now that the text has been preprocessed, we can count how many times each word in the text appears in our positive and negative word lists. This will allow us to calculate the sentiment score.\n",
    "\n",
    "## Counting Word Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9427ba8-0418-44dd-acbe-f2d5c9b0b551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive words count: 6\n",
      "Negative words count: 1\n"
     ]
    }
   ],
   "source": [
    "# Split the preprocessed text into individual words (tokens)\n",
    "words = set(preprocessed_text.split())\n",
    "\n",
    "# Count occurrences of positive words\n",
    "positive_count = len(words.intersection(positive_words))\n",
    "\n",
    "# Count occurrences of negative words\n",
    "negative_count = len(words.intersection(negative_words))\n",
    "\n",
    "# Print results\n",
    "print(f\"Positive words count: {positive_count}\")\n",
    "print(f\"Negative words count: {negative_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d9d3f-6d0b-4a34-9381-e71fbd66ac07",
   "metadata": {},
   "source": [
    "In this step, we use Python's set.intersection() method to count how many words from the text match the words in our positive and negative word lists. We count each set of words and print the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bc47b-bc03-488d-b283-f59aa27dca3e",
   "metadata": {},
   "source": [
    "## Calculating the Sentiment Score Using Percentages\n",
    "\n",
    "To get a more balanced view of the sentiment in the text, we can calculate the percentage of positive and negative words relative to the total number of words in the preprocessed text. This will give us a clearer indication of the sentiment distribution, especially when analyzing longer texts.\n",
    "\n",
    "### Sentiment Ratio Explanation\n",
    "\n",
    "Instead of simply counting positive and negative words, we calculate the ratio of these words as a percentage of the total words in the text. The formulas we will use are:\n",
    "\n",
    "- **Positive Sentiment Ratio** = (Number of Positive Words / Total Words) * 100\n",
    "- **Negative Sentiment Ratio** = (Number of Negative Words / Total Words) * 100\n",
    "\n",
    "This allows us to standardize the score regardless of the length of the text. A higher percentage indicates a stronger presence of positive or negative sentiment.\n",
    "\n",
    "We can also calculate a **Combined Sentiment Score**:\n",
    "- **Combined Sentiment Score** = Positive Sentiment Ratio - Negative Sentiment Ratio\n",
    "\n",
    "This will give us a single score that represents the overall tone of the text. A positive score suggests an overall positive tone, while a negative score suggests a negative tone.\n",
    "\n",
    "---\n",
    "\n",
    "### Calculating the Positive and Negative Sentiment Ratios\n",
    "\n",
    "Let's now calculate the sentiment ratios for our dummy earnings call text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca8801b3-f324-424e-bd75-d36337b82a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentiment Ratio: 31.58%\n",
      "Negative Sentiment Ratio: 5.26%\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of words in the preprocessed text\n",
    "total_words = len(preprocessed_text.split())\n",
    "\n",
    "# Calculate the positive sentiment ratio (as a percentage of total words)\n",
    "positive_ratio = (positive_count / total_words) * 100\n",
    "\n",
    "# Calculate the negative sentiment ratio (as a percentage of total words)\n",
    "negative_ratio = (negative_count / total_words) * 100\n",
    "\n",
    "# Print the ratios\n",
    "print(f\"Positive Sentiment Ratio: {positive_ratio:.2f}%\")\n",
    "print(f\"Negative Sentiment Ratio: {negative_ratio:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e0f3d-db82-4183-bb2f-31f70d097723",
   "metadata": {},
   "source": [
    "This gives us the percentage of positive and negative words in the text relative to the total number of words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6631320-77b7-4266-b04f-686f9b242a4f",
   "metadata": {},
   "source": [
    "## Combined Sentiment Score\n",
    "The combined sentiment score is calculated by subtracting the negative sentiment ratio from the positive sentiment ratio. This single value indicates whether the text has an overall positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98cab569-08d2-4d7d-babf-293ce7e1641d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Sentiment Score: 26.32\n"
     ]
    }
   ],
   "source": [
    "# Calculate the combined sentiment score\n",
    "combined_sentiment_score = positive_ratio - negative_ratio\n",
    "\n",
    "# Print the combined sentiment score\n",
    "print(f\"Combined Sentiment Score: {combined_sentiment_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9153cf-4bc5-4e62-8c95-c742b81e5be6",
   "metadata": {},
   "source": [
    "This combined score helps us understand the overall sentiment of the text. A positive combined score indicates that the text has more positive sentiment than negative sentiment, while a negative score indicates the opposite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de989cf2-3c46-40e6-9c15-21dce4aab9c6",
   "metadata": {},
   "source": [
    "# Example Results\n",
    "Let’s assume that our dummy earnings call text has the following counts and total words:\n",
    "\n",
    "Positive Words Count: 4\n",
    "Negative Words Count: 2\n",
    "Total Words: 30\n",
    "The sentiment scores would be:\n",
    "\n",
    "Positive Sentiment Ratio: (4 / 30) * 100 = 13.33%\n",
    "Negative Sentiment Ratio: (2 / 30) * 100 = 6.67%\n",
    "Combined Sentiment Score: 13.33% - 6.67% = 6.66%\n",
    "In this case, the text has a generally positive sentiment, as indicated by the positive combined score.\n",
    "\n",
    "By calculating sentiment ratios as a percentage of the total text, we get a clearer and more standardized view of the sentiment, making it easier to compare texts of different lengths. This method is especially useful in finance, where the tone of financial reports, news articles, and earnings calls can significantly influence decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "- **Positive and Negative Sentiment Ratios**: Explained the concept of calculating sentiment ratios as a percentage of the total word count, which allows for standardization regardless of the length of the text.\n",
    "- **Combined Sentiment Score**: Provided a formula and code to calculate a single score by subtracting the negative ratio from the positive ratio, giving a clear indication of the overall sentiment.\n",
    "- **Code**: The code calculates and prints the positive and negative sentiment ratios as percentages, along with the combined score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9f6d9-0adc-4628-a124-62184666f4ea",
   "metadata": {},
   "source": [
    "# EXTRA:\n",
    "## Understanding SpaCy Language Models: `en_core_web_sm` vs `en_core_web_md`\n",
    "\n",
    "When using `spaCy` for NLP tasks, you’ll encounter different language models, such as `en_core_web_sm` and `en_core_web_md`. These models differ in size, the amount of data they are trained on, and their performance in various NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### What Are SpaCy Language Models?\n",
    "\n",
    "A **spaCy language model** is a pre-trained model that has learned the linguistic patterns and features of a particular language (in our case, English). These models are used to perform a wide range of NLP tasks, such as:\n",
    "- **Tokenization**: Splitting text into words or tokens.\n",
    "- **Part-of-Speech Tagging**: Identifying the grammatical role of each word (e.g., noun, verb, adjective).\n",
    "- **Lemmatization**: Reducing words to their base form (e.g., \"running\" becomes \"run\").\n",
    "- **Named Entity Recognition (NER)**: Detecting entities like names, dates, and organizations in the text.\n",
    "\n",
    "---\n",
    "\n",
    "### `en_core_web_sm` (Small Model)\n",
    "\n",
    "`en_core_web_sm` is the **small** version of spaCy’s English language model. Here are the key characteristics of this model:\n",
    "\n",
    "- **Size**: Small (~50 MB).\n",
    "- **Speed**: Fast because of its small size, making it suitable for tasks where quick processing is needed.\n",
    "- **Accuracy**: Reasonably accurate for basic tasks like tokenization, lemmatization, and part-of-speech tagging.\n",
    "- **Limitations**: Since it's smaller, the model has fewer parameters and may not perform as well on more complex tasks like Named Entity Recognition (NER). It also lacks word vectors, which means it can’t understand nuanced relationships between words in the same way larger models can.\n",
    "\n",
    "This model is ideal for **quick testing** and situations where you don't need deep contextual understanding of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9082ead3-2eb8-4aa4-90dc-ecdd58dfbc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the small language model\n",
    "#import spacy\n",
    "#nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fc49ac-14d6-40a9-9f8c-f170c4e6e926",
   "metadata": {},
   "source": [
    "### `en_core_web_md` (Medium Model)\n",
    "\n",
    "en_core_web_md is the medium version of spaCy’s English language model, and it comes with a number of improvements over the small model.\n",
    "\n",
    "Size: Medium (~100 MB), larger than the small model.\n",
    "Speed: Slightly slower than en_core_web_sm due to the larger size, but still efficient.\n",
    "Accuracy: More accurate than the small model, especially for tasks like Named Entity Recognition (NER).\n",
    "Word Vectors: This model includes word vectors, which means it can better capture the meaning and relationships between words. This can be useful for tasks like sentiment analysis, where understanding the context and relationships between words is important.\n",
    "Because en_core_web_md has word vectors, it can handle more nuanced tasks, and its performance is generally better for more complex NLP tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb336e-7ef2-47c7-946e-8c25a1028afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the medium language model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03279f-234d-4349-b993-a295f8ec1660",
   "metadata": {},
   "source": [
    "# When to Use Each Model\n",
    "### Use en_core_web_sm if:\n",
    "\n",
    "You are working with smaller datasets or running quick prototypes.\n",
    "You don’t need very detailed linguistic analysis.\n",
    "Speed and efficiency are more important than accuracy.\n",
    "\n",
    "### Use en_core_web_md if:\n",
    "You need better accuracy, especially for tasks like Named Entity Recognition (NER).\n",
    "You are working with text where word meaning and context matter more (e.g., sentiment analysis).\n",
    "You need word vectors to understand relationships between words.\n",
    "\n",
    "### Larger Models: en_core_web_lg\n",
    "There is also a larger model, en_core_web_lg, which is even more powerful:\n",
    "\n",
    "Size: Large (~800 MB).\n",
    "Accuracy: Very high due to more parameters and better word vectors.\n",
    "Word Vectors: Full word vectors, meaning it can capture deep contextual relationships between words.\n",
    "This model is ideal for large-scale NLP tasks where accuracy is critical and you have the resources to handle its larger size.\n",
    "\n",
    "However, in many cases, en_core_web_md is a good balance between speed and accuracy for common NLP tasks, especially in finance, where you need both good performance and reasonable processing time.\n",
    "\n",
    "## Conclusion: Which Model to Use?\n",
    "For this course, where we are primarily focused on basic text preprocessing, sentiment analysis, and simple dictionary-based methods, en_core_web_sm will be sufficient. It is fast, efficient, and can handle our preprocessing tasks (lemmatization, stop word removal, etc.) with ease.\n",
    "\n",
    "If you later move on to more complex NLP tasks that require better accuracy and deeper understanding of the text (like Named Entity Recognition or advanced sentiment models), consider upgrading to en_core_web_md or even en_core_web_lg.\n",
    "\n",
    "By choosing the appropriate model, you can balance performance, accuracy, and speed based on your needs.\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "- **`en_core_web_sm`**: A lightweight, fast model for basic NLP tasks, explained in terms of its size and limitations. I recommended this for quick, simple tasks like preprocessing and basic dictionary-based sentiment analysis.\n",
    "- **`en_core_web_md`**: A medium-sized model with word vectors, better suited for tasks that require a deeper understanding of words and context, such as more advanced sentiment analysis or Named Entity Recognition (NER).\n",
    "- **When to Use Each Model**: Clearly explained when to use each model based on the type of NLP task and resource requirements.\n",
    "- **Conclusion**: The small model is ideal for the current level of the course, but the medium model can be used for more complex tasks if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2541a77-1613-487c-97c1-c7ae5a0a9825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
